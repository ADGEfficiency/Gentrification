{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load('./dataframes/final_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['year']>=2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert year to datetime object and set as index\n",
    "\n",
    "df['year'] = pd.to_datetime(df['year'], format='%Y')\n",
    "df.set_index('year', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column Bezirksname\n",
    "\n",
    "df.drop('Bezirksname', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummies for urban_style\n",
    "\n",
    "dummies = pd.get_dummies(df['urban_style'])\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "df.drop('urban_style', axis=1, inplace=True)\n",
    "df.rename(columns={'Einfamilienhausgebiete': 'urban_style_family',\n",
    "                    'GroÃŸsiedlungen der 60er - 80er Jahre': 'urban_style_60s',\n",
    "                    'Siedlungsbau der 20er - 30er Jahre': 'urban_style_20s',\n",
    "                    'Siedlungsbau der 50er Jahre': 'urban_style_50s',\n",
    "                    'Siedlungsbau der 90er - 2000er Jahre': 'urban_style_90s',\n",
    "                    'Verdichtete Blockrandbebauung': 'urban_style_old'\n",
    "                    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in train, cv and test set\n",
    "\n",
    "train_df = df[(df.index>='2010-01-01') & (df.index<'2017-01-01')]\n",
    "val_df = df[(df.index=='2017-01-01') | (df.index=='2018-01-01')]\n",
    "test_df = df[df.index=='2019-01-01']\n",
    "\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, label=None):\n",
    "    X = df\n",
    "    \n",
    "    if label:\n",
    "        X = df.drop(label, axis=1)\n",
    "        y = df[label]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(df, train_df=df, mode='train'):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    if mode=='train':\n",
    "        df_scaled = scaler.fit_transform(df)\n",
    "        \n",
    "    else:\n",
    "        scaler.fit_transform(train_df)\n",
    "        df_scaled = scaler.transform(df)\n",
    "        \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on sqm_price_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets in features and target\n",
    "\n",
    "X_train, y_train = split_dataset(train_df, label='sqm_price_all')\n",
    "X_cv, y_cv = split_dataset(val_df, label='sqm_price_all')\n",
    "X_test, y_test = split_dataset(test_df, label='sqm_price_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale dataframes\n",
    "\n",
    "scaled_X_train = scaling(X_train, 'train')\n",
    "scaled_X_cv = scaling(X_cv, X_train, 'cv')\n",
    "scaled_X_test = scaling(X_test, X_train, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.030933482829500456\n",
      "R2 Score: 0.9885529534608014\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(scaled_X_train, y_train)\n",
    "predictions = reg.predict(scaled_X_cv)\n",
    "\n",
    "print('Mean Squared Error: {}'.format(str(mean_squared_error(np.array(y_cv), predictions))))\n",
    "print('R2 Score: {}'.format(str(r2_score(np.array(y_cv), predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual gridsearch\n",
    "\n",
    "def grid_search_estimators(train_X, cv_X, train_y, cv_y):\n",
    "    start = time.time()\n",
    "\n",
    "    #Grid Search over n_estimators\n",
    "    for i in [500, 1000]:\n",
    "        \n",
    "        #Initialize XGB\n",
    "        reg = GradientBoostingRegressor(n_estimators=i)\n",
    "\n",
    "        reg.fit(train_X, train_y)\n",
    "        predictions = reg.predict(cv_X)\n",
    "\n",
    "        result = 'n_estimators: {}'.format(i)\n",
    "        print(result)\n",
    "        print('-'*len(result))\n",
    "        print('Mean Squared Error: {}'.format(str(mean_squared_error(np.array(cv_y), predictions))))\n",
    "        print('R2 Score: {}'.format(str(r2_score(np.array(cv_y), predictions))))\n",
    "        print('\\n')\n",
    "\n",
    "    print(f'Time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 500\n",
      "-----------------\n",
      "Mean Squared Error: 0.3378950891252702\n",
      "R2 Score: 0.8749607074023064\n",
      "\n",
      "\n",
      "n_estimators: 1000\n",
      "------------------\n",
      "Mean Squared Error: 0.33422907421428577\n",
      "R2 Score: 0.8763173294008941\n",
      "\n",
      "\n",
      "Time: 43.93093490600586\n"
     ]
    }
   ],
   "source": [
    "grid_search_estimators(scaled_X_train, scaled_X_cv, y_train, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_depth(train_X, cv_X, train_y, cv_y, n_estimators):\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(4, 10, 2):\n",
    "        #Initialize XGB\n",
    "        reg = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=i)\n",
    "\n",
    "        reg.fit(train_X, train_y)\n",
    "        predictions = reg.predict(cv_X)\n",
    "\n",
    "        result = 'n_estimators: {} | max_depth: {}'.format(n_estimators, i)\n",
    "        print(result)\n",
    "        print('-'*len(result))\n",
    "        print('Mean Squared Error: {}'.format(str(mean_squared_error(np.array(cv_y), predictions))))\n",
    "        print('R2 Score: {}'.format(str(r2_score(np.array(cv_y), predictions))))\n",
    "        print('\\n')\n",
    "\n",
    "    print(f'Time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 1000 | max_depth: 4\n",
      "---------------------------------\n",
      "Mean Squared Error: 0.32857035906540605\n",
      "R2 Score: 0.8784113572870628\n",
      "\n",
      "\n",
      "n_estimators: 1000 | max_depth: 6\n",
      "---------------------------------\n",
      "Mean Squared Error: 0.38590473056252494\n",
      "R2 Score: 0.857194566974744\n",
      "\n",
      "\n",
      "n_estimators: 1000 | max_depth: 8\n",
      "---------------------------------\n",
      "Mean Squared Error: 0.3982570463682638\n",
      "R2 Score: 0.8526235480993549\n",
      "\n",
      "\n",
      "Time: 159.90856099128723\n"
     ]
    }
   ],
   "source": [
    "grid_search_depth(scaled_X_train, scaled_X_cv, y_train, y_cv, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_gridsearch(train_X, train_y, test_X, test_y, n_splits=3, n_jobs=-1):\n",
    "    '''\n",
    "    Function performs GridSearch using TimeSeries CV\n",
    "    X_train, y_train\n",
    "    n_splits=number of splits in TimeSeriesCV; default:5\n",
    "    n_jobs=default: -1\n",
    "    '''\n",
    "    \n",
    "    model = GradientBoostingRegressor()\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    gsearch = GridSearchCV(estimator=model, cv=tscv,\n",
    "                            param_grid=params, n_jobs=n_jobs)\n",
    "\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best params were: {}\".format(gsearch.best_params_))\n",
    "    print('\\n')\n",
    "    \n",
    "    best_model = gsearch.best_estimator_\n",
    "    \n",
    "    print('Mean Squared Error on cv set: {}'.format(str(mean_squared_error(np.array(test_y), best_model.predict(test_X)))))\n",
    "    print('R2 Score on cv set: {}'.format(str(r2_score(np.array(test_y), best_model.predict(test_X)))))\n",
    "    print('\\n')\n",
    "    print('Mean Squared Error on train set: {}'.format(str(mean_squared_error(np.array(train_y), best_model.predict(train_X)))))\n",
    "    print('R2 Score on train set: {}'.format(str(r2_score(np.array(train_y), best_model.predict(train_X)))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [2, 3, 4]                   #depth of trees\n",
    "n_estimators = [25, 50, 75, 100, 150]   #number of base learners\n",
    "\n",
    "params = {\n",
    "    'max_depth': max_depth,\n",
    "    'n_estimators': n_estimators,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params were: {'max_depth': 4, 'n_estimators': 50}\n",
      "\n",
      "\n",
      "Mean Squared Error on cv set: 20.635275080427967\n",
      "R2 Score on cv set: -6.636157735512235\n",
      "\n",
      "\n",
      "Mean Squared Error on train set: 7.876427299684027\n",
      "R2 Score on train set: -1.921652613603852\n"
     ]
    }
   ],
   "source": [
    "xgb_gridsearch(scaled_X_train, y_train, scaled_X_cv, y_cv, n_splits=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
